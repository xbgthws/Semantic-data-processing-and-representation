{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yansimei/anaconda3/envs/NLP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yansimei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yansimei/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_data.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        line = line.split(' ::: ')\n",
    "        data.append(line)\n",
    "    df = pd.DataFrame(data, columns=['id', 'title', 'genre', 'comment'])\n",
    "    df.to_csv('data/movie_comment.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Oscar et la dame rose (2009)</td>\n",
       "      <td>drama</td>\n",
       "      <td>Listening in to a conversation between his doc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Cupid (1997)</td>\n",
       "      <td>thriller</td>\n",
       "      <td>A brother and sister with a past incestuous re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Young, Wild and Wonderful (1980)</td>\n",
       "      <td>adult</td>\n",
       "      <td>As the bus empties the students for their fiel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Secret Sin (1915)</td>\n",
       "      <td>drama</td>\n",
       "      <td>To help their unemployed father make ends meet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Unrecovered (2007)</td>\n",
       "      <td>drama</td>\n",
       "      <td>The film's title refers not only to the un-rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             title     genre  \\\n",
       "0   1      Oscar et la dame rose (2009)     drama   \n",
       "1   2                      Cupid (1997)  thriller   \n",
       "2   3  Young, Wild and Wonderful (1980)     adult   \n",
       "3   4             The Secret Sin (1915)     drama   \n",
       "4   5            The Unrecovered (2007)     drama   \n",
       "\n",
       "                                             comment  \n",
       "0  Listening in to a conversation between his doc...  \n",
       "1  A brother and sister with a past incestuous re...  \n",
       "2  As the bus empties the students for their fiel...  \n",
       "3  To help their unemployed father make ends meet...  \n",
       "4  The film's title refers not only to the un-rec...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/movie_comment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = df['title'].values\n",
    "genre = df['genre'].values\n",
    "comment = df['comment'].values\n",
    "\n",
    "# 删除特殊字符、标点符号\n",
    "title = [re.sub(r'[^\\w\\s]', '', t) for t in title]\n",
    "comment = [re.sub(r'[^\\w\\s]', '', c) for c in comment]\n",
    "# 删除数字\n",
    "title = [re.sub(r'\\d+', '', t) for t in title]\n",
    "comment = [re.sub(r'\\d+', '', c) for c in comment]\n",
    "# 英文字符全部转换为小写\n",
    "title = [t.lower() for t in title]\n",
    "comment = [c.lower() for c in comment]\n",
    "# 多个空格转换为一个空格\n",
    "title = [re.sub(r'\\s+', ' ', t) for t in title]\n",
    "comment = [re.sub(r'\\s+', ' ', c) for c in comment]\n",
    "# 删除首尾空格\n",
    "title = [t.strip() for t in title]\n",
    "comment = [c.strip() for c in comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除停用词\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "title = [' '.join([w for w in t.split() if w not in stopwords]) for t in title]\n",
    "comment = [' '.join([w for w in c.split() if w not in stopwords]) for c in comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oscar et la dame rose',\n",
       " 'cupid',\n",
       " 'young wild wonder',\n",
       " 'secret sin',\n",
       " 'unrecov']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词干提取\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "title = [' '.join([ps.stem(w) for w in t.split()]) for t in title]\n",
    "comment = [' '.join([ps.stem(w) for w in c.split()]) for c in comment]\n",
    "# 词形还原（Lemmatization）\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "title = [' '.join([wnl.lemmatize(w) for w in t.split()]) for t in title]\n",
    "comment = [' '.join([wnl.lemmatize(w) for w in c.split()]) for c in comment]\n",
    "\n",
    "title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oscar et la dame rise',\n",
       " 'cupid',\n",
       " 'young wild wonder',\n",
       " 'secret sin',\n",
       " 'unrecov']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词性还原\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # 默认情况下返回 'NOUN'\n",
    "\n",
    "title = [pos_tag(t.split()) for t in title]\n",
    "comment = [pos_tag(c.split()) for c in comment]\n",
    "title = [' '.join([wnl.lemmatize(w[0], get_wordnet_pos(w[1])) for w in t]) for t in title]\n",
    "comment = [' '.join([wnl.lemmatize(w[0], get_wordnet_pos(w[1])) for w in c]) for c in comment]\n",
    "\n",
    "title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oscar et la dame rise',\n",
       " 'cupid',\n",
       " 'young wild wonder',\n",
       " 'secret sin',\n",
       " 'unrecov']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过 WordNet 修正文本中的单词\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def get_correct_word(word):\n",
    "    # 如果单词存在于 WordNet 词汇库中，则直接返回该单词\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    # 如果单词不存在于 WordNet 中，则查找具有最小编辑距离的候选单词，并返回该候选单词\n",
    "    else:\n",
    "        candidates = set()\n",
    "        for w in wordnet.synsets(word):\n",
    "            for lemma in w.lemmas():\n",
    "                candidates.add(lemma.name())\n",
    "        if not candidates:\n",
    "            return word\n",
    "        else:\n",
    "            return max(candidates, key=lambda x: edit_distance(word, x))\n",
    "title = [' '.join([get_correct_word(w) for w in t.split()]) for t in title]\n",
    "comment = [' '.join([get_correct_word(w) for w in c.split()]) for c in comment]\n",
    "\n",
    "title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, title, comment, genre, tokenizer, max_length):\n",
    "        self.title = title\n",
    "        self.comment = comment\n",
    "        self.genre = genre\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 使用LabelEncoder对genre进行编码\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.genre_encoded = self.label_encoder.fit_transform(self.genre)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = f\"{self.title[idx]} {self.comment[idx]}\"\n",
    "        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        label = self.genre_encoded[idx]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# 训练测试划分\n",
    "title_train, title_test, comment_train, comment_test, genre_train, genre_test = train_test_split(\n",
    "    title, comment, genre, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 128\n",
    "\n",
    "# 创建训练数据集和 DataLoader\n",
    "train_dataset = TextDataset(title_train, comment_train, genre_train, tokenizer, max_length)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 创建测试数据集和 DataLoader\n",
    "test_dataset = TextDataset(title_test, comment_test, genre_test, tokenizer, max_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 440M/440M [01:15<00:00, 5.84MB/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/yansimei/anaconda3/envs/NLP/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5 - Training: 100%|██████████| 1186/1186 [1:59:16<00:00,  6.03s/it]\n",
      "Epoch 1/5 - Testing: 100%|██████████| 509/509 [14:50<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Train Loss: 0.0522, Train Accuracy: 0.5231, Test Loss: 0.0421, Test Accuracy: 0.6037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Training: 100%|██████████| 1186/1186 [1:46:22<00:00,  5.38s/it]\n",
      "Epoch 2/5 - Testing: 100%|██████████| 509/509 [13:08<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Train Loss: 0.0376, Train Accuracy: 0.6438, Test Loss: 0.0397, Test Accuracy: 0.6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Training: 100%|██████████| 1186/1186 [1:40:28<00:00,  5.08s/it]\n",
      "Epoch 3/5 - Testing: 100%|██████████| 509/509 [12:44<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - Train Loss: 0.0295, Train Accuracy: 0.7175, Test Loss: 0.0402, Test Accuracy: 0.6165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Training: 100%|██████████| 1186/1186 [1:38:53<00:00,  5.00s/it]\n",
      "Epoch 4/5 - Testing: 100%|██████████| 509/509 [12:44<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Train Loss: 0.0219, Train Accuracy: 0.7944, Test Loss: 0.0440, Test Accuracy: 0.6125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Training: 100%|██████████| 1186/1186 [1:38:33<00:00,  4.99s/it]\n",
      "Epoch 5/5 - Testing: 100%|██████████| 509/509 [12:31<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - Train Loss: 0.0152, Train Accuracy: 0.8607, Test Loss: 0.0485, Test Accuracy: 0.6017\n",
      "Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义模型\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(train_dataset.genre)))\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 训练参数\n",
    "num_epochs = 5\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Training'):\n",
    "        inputs = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        targets = batch['label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        correct_train += (outputs.logits.argmax(dim=1) == targets).sum().item()\n",
    "\n",
    "    average_train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "\n",
    "    # 在测试集上评估模型\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Testing'):\n",
    "            inputs = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            targets = batch['label']\n",
    "\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=targets)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            correct_test += (outputs.logits.argmax(dim=1) == targets).sum().item()\n",
    "\n",
    "    average_test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_accuracy = correct_test / len(test_loader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "          f'Train Loss: {average_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "          f'Test Loss: {average_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "print('Training finished.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
